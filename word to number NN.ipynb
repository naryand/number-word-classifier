{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Flatten, Reshape\n",
    "high = 1000000\n",
    "digits = len(str(high))\n",
    "pad = 10\n",
    "\n",
    "from num2words import num2words\n",
    "import random\n",
    "\n",
    "def index_list(pos):\n",
    "    index_list = [0] * (pos)\n",
    "    index_list.append(1)\n",
    "    index_list += [0] * (10-pos-1)\n",
    "    return index_list\n",
    "\n",
    "def create_data(low,high, num):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(num):\n",
    "        a = random.randrange(low, high)\n",
    "        b = a\n",
    "        words = num2words(b)\n",
    "        c = str(b).zfill(digits)  \n",
    "        x_data.append(words.replace(\"-\", \" \").replace(\",\", \"\").replace(\" and \",\" \"))\n",
    "        num_list = []\n",
    "        for i in range(digits):\n",
    "            num_list.append(index_list(int(c[i])))\n",
    "        y_data.append(num_list)\n",
    "    return x_data, np.array(y_data)\n",
    "\n",
    "def append_data(x, y, x_data, y_data):\n",
    "    x_data.append(x.replace(\"-\", \" \").replace(\",\", \"\").replace(\" and \",\" \"))\n",
    "    c = str(y).zfill(digits) \n",
    "    num_list = [[]]\n",
    "    for i in range(digits):\n",
    "        num_list[0].append(index_list(int(c[i])))\n",
    "    num_list = np.array(num_list)\n",
    "    y_data = np.concatenate((y_data, num_list), axis=0)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = create_data(0, 1000000, 60000)\n",
    "x_test, y_test = create_data(0, 1000000, 40000)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(1000, 1500, 100):\n",
    "        x_train, y_train = append_data(num2words(j//100) + \" hundred\", j, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three hundred two thousand one hundred thirty eight\n",
      "[ 0  0  6  1  8  2  3  1 14  9]\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "for i in x_train:\n",
    "    num_words += len(i.split(\" \"))\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = np.array([[0]*(pad-len(i)) + i for i in x_train])\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = np.array([[0]*(pad-len(i)) + i for i in x_test])\n",
    "\n",
    "print(tokenizer.sequences_to_texts(x_train)[342])\n",
    "print(x_train[342])\n",
    "print(y_train[342])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 10)            300       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 70)                7070      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 10)             0         \n",
      "=================================================================\n",
      "Total params: 7,370\n",
      "Trainable params: 7,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "60050/60050 [==============================] - 2s 30us/sample - loss: 0.6573 - acc: 0.8552\n",
      "Epoch 2/35\n",
      "60050/60050 [==============================] - 2s 28us/sample - loss: 0.1040 - acc: 0.9828\n",
      "Epoch 3/35\n",
      "60050/60050 [==============================] - 2s 28us/sample - loss: 0.0624 - acc: 0.9889\n",
      "Epoch 4/35\n",
      "60050/60050 [==============================] - 2s 28us/sample - loss: 0.0448 - acc: 0.9915\n",
      "Epoch 5/35\n",
      "60050/60050 [==============================] - 2s 27us/sample - loss: 0.0343 - acc: 0.9934\n",
      "Epoch 6/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0272 - acc: 0.9947\n",
      "Epoch 7/35\n",
      "60050/60050 [==============================] - 1s 25us/sample - loss: 0.0223 - acc: 0.9955\n",
      "Epoch 8/35\n",
      "60050/60050 [==============================] - 2s 25us/sample - loss: 0.0191 - acc: 0.9962\n",
      "Epoch 9/35\n",
      "60050/60050 [==============================] - 2s 25us/sample - loss: 0.0167 - acc: 0.9967\n",
      "Epoch 10/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0151 - acc: 0.9972\n",
      "Epoch 11/35\n",
      "60050/60050 [==============================] - 2s 25us/sample - loss: 0.0139 - acc: 0.9975\n",
      "Epoch 12/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0129 - acc: 0.9978\n",
      "Epoch 13/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0122 - acc: 0.9979\n",
      "Epoch 14/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0115 - acc: 0.9981\n",
      "Epoch 15/35\n",
      "60050/60050 [==============================] - 2s 25us/sample - loss: 0.0110 - acc: 0.9982\n",
      "Epoch 16/35\n",
      "60050/60050 [==============================] - 1s 25us/sample - loss: 0.0106 - acc: 0.9983\n",
      "Epoch 17/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 18/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0100 - acc: 0.9985\n",
      "Epoch 19/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0098 - acc: 0.9985\n",
      "Epoch 20/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0096 - acc: 0.9985\n",
      "Epoch 21/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0094 - acc: 0.9986\n",
      "Epoch 22/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0092 - acc: 0.9986\n",
      "Epoch 23/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0091 - acc: 0.9986\n",
      "Epoch 24/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0090 - acc: 0.9987\n",
      "Epoch 25/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0089 - acc: 0.9987\n",
      "Epoch 26/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0087 - acc: 0.9987\n",
      "Epoch 27/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0087 - acc: 0.9987\n",
      "Epoch 28/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0086 - acc: 0.9987\n",
      "Epoch 29/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0085 - acc: 0.9987\n",
      "Epoch 30/35\n",
      "60050/60050 [==============================] - 2s 27us/sample - loss: 0.0085 - acc: 0.9987\n",
      "Epoch 31/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0084 - acc: 0.9987\n",
      "Epoch 32/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0084 - acc: 0.9987\n",
      "Epoch 33/35\n",
      "60050/60050 [==============================] - 2s 26us/sample - loss: 0.0083 - acc: 0.9988\n",
      "Epoch 34/35\n",
      "60050/60050 [==============================] - 2s 28us/sample - loss: 0.0083 - acc: 0.9988\n",
      "Epoch 35/35\n",
      "60050/60050 [==============================] - 2s 27us/sample - loss: 0.0082 - acc: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21935e59a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(Embedding(vocab_size, pad, input_length=pad))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(digits*10, activation=tf.nn.softmax))\n",
    "model.add(Reshape((digits,10)))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 15us/sample - loss: 0.0100 - acc: 0.9986\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('count2.model')\n",
    "new_model = tf.keras.models.load_model('count2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-75521ba277f0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-75521ba277f0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    ,\"two hundred thirty seven thousand one hundred forty\")\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = tokenizer.texts_to_sequences([\"twelve\"\n",
    "                                 ,\"thirteen\"\n",
    "                                 ,\"one hundred twenty three\"\n",
    "                                 ,\"four hundred seventy two thousand two hundred twenty two\"\n",
    "                                 ,\"two hundred thirty seven thousand one hundred forty\")\n",
    "                                 ,\"spongebob\"])\n",
    "x = np.array([[0]*(pad-len(i)) + i for i in x])\n",
    "predictions = new_model.predict(np.array(x))\n",
    "\n",
    "for prediction in predictions:\n",
    "    num = \"\"\n",
    "    for i in prediction:\n",
    "        num += str(i.argmax())\n",
    "    print(int(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
